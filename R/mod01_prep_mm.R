#######################################
### Worflow functions               ###
### for EcoservR                    ###
### Sandra Angers-Blondin           ###
### 05 October 2020                 ###
#######################################

#' Prepare MasterMap
#'
#' This function imports the OS MasterMap from its specified folder.

#' @param projectLog The RDS project log file generated by the wizard app and containing all file paths to data inputs and model parameters
#' @return Saves a project_title_MM_01.RDS file to project folder
#' @export
#'
prepare_basemap <- function(projectLog = parent.frame()$projectLog){
# the parent frame bit makes sure the function knows that the argument comes from the users's environment

   timeA <- Sys.time() # start time

## Extract the file paths and other info from project log -----

   mmpath <- projectLog$df[projectLog$df$dataset == "mm", ][["path"]]
   mmlayer <- projectLog$df[projectLog$df$dataset == "mm", ][["layer"]]
   mm_cols <- projectLog$df[projectLog$df$dataset == "mm", ][["cols"]][[1]] # coerce to named character (remove list nesting)

   studypath <- projectLog$df[projectLog$df$dataset == "studyArea", ][["path"]]
   studybuffer <- projectLog$parameters$SAbuffer

   output_temp <- projectLog$output_temp
   title <- projectLog$title

   ### Fist check colnames are valid:
   checkNames(mmpath, mmlayer, mm_cols) # will stop execution if col names specified by user don't match names in layer


   ### Create output folder if doesn't exist
   if (!dir.exists(output_temp)){
      dir.create(output_temp)
   }


   ### Import the study area outline (specifying OSGB as crs)

   studyArea <- try(loadSpatial(studypath,
                                layer = NULL,
                                filetype = guessFiletype(studypath)) %>%
                       do.call(rbind, .))

   if (inherits(studyArea, "try-error")) stop("Could not import study area. Check file path and format.") else message("Study area imported.")

   studyArea <- suppressWarnings({
      checkcrs(studyArea, 27700) %>%   # check that CRS is Brit National Grid, and transform if not
         sf::st_set_crs(., 27700)  # set the crs manually to get rid of GDAL errors with init string format
   })



### Import the OS Master Map

   # NOTE: Most users will have done a data download that has many files, one for each 10km tile.
   # All the files should be in the same folder for the function to work (the name of the folder is the first argument of the function).
   # The loadSpatial function creates a list of all the TopographicArea layers available in the folder. It is this list-object that gets updated sequentially during the next steps.

   message("Preparing to load Master Map")
   mm <- try(loadSpatial(folder = mmpath,
                         layer = mmlayer,
                         filetype = guessFiletype(mmpath)))

   if (inherits(mm, "try-error")) stop("Could not import OS Mastermap. Check file path and format.")

   message("Master Map imported.")

   ## Check and set crs

   mm <- checkcrs(mm, 27700)

   # DATA PREP -----------------------------------------------------------------------------------

   ### Step 1. Create a buffer around the studyArea -----

   studyAreaBuffer <- sf::st_buffer(studyArea, studybuffer) %>% # create a buffer around the study area shape
      sf::st_make_valid() %>% # make sure outline is valid
      sf::st_geometry() %>% sf::st_as_sf()  # retain only geometry

   rm(studyArea)  # remove original boundary

   # Save the buffer to disk for next scripts

   # Save the buffer as a geopackage
   sf::st_write(studyAreaBuffer,
            dsn = file.path(output_temp,
                            paste(title, "_studyAreaBuffer_",studybuffer,"m.gpkg", sep="")),
            append = FALSE # will overwrite existing object if present
   )


   # and as R object for easy handling - recalling in other modules
   saveRDS(studyAreaBuffer, file.path(output_temp, paste0(title,"_studyAreaBuffer.RDS")))

   if(sf::st_is_valid(studyAreaBuffer)) {message("Study area buffered and saved to your output folder.")}



   message("Tidying up the master map")

   ### Step 2. Tidy the MM to keep only needed fields -----

   ### Rename columns in case they were not the OS default

   for (i in 1:length(mm)){
      mm[[i]] <- dplyr::rename(mm[[i]], !!mm_cols)
   }

   ### Filter out features we don't need
   mm <- lapply(mm,
                function(x)
                   dplyr::filter(x,
                                 PhysicalLevel != "51",  # remove things above ground level
                                 !grepl("Landform", x$Group)  # remove any group with landform in description
                   ) %>%
                   dplyr::select(-PhysicalLevel)

   )

   ### Step 3. Crop the MM polygons to the study area -----
   ### And assign to grid reference

   ## We use the OS 10km grid tiles to chop up a large basemap or reshuffle existing tiles slightly. This results in a list of tiles where each is named according to its 10km grid ref.

   # Grid is a dataset built into the package and should be present without having to be called

   # grid <- st_read("builtin/OSgrid10km", layer = "grid10km")  # the 10km national grid in the builtin/ folder
   grid <- suppressMessages(checkcrs(grid, studyAreaBuffer))  # reproject if needed


   # Set crs for list elements so they match buffer (OSGB36) -they should already but just in case

   mm <- suppressWarnings({
      lapply(mm, function(x) checkcrs(x, studyAreaBuffer) %>%
                sf::st_set_crs(27700))
   })


   ### Narrow grid down to study area

   # Only keep grid squares that intersect the study area
   grid <- grid[lengths(sf::st_intersects(grid, studyAreaBuffer)) > 0,]

   # Drop unused factor levels
   if (inherits(grid$TILE_NAME, "factor")){
      grid$TILE_NAME <- droplevels(grid$TILE_NAME)
   }

   # Clip grid squares to study area (+buffer)
   grid <- suppressWarnings(sf::st_intersection(grid, sf::st_geometry(studyAreaBuffer)))


   ## At this stage we have either one large mastermap that needs clipping, or any number of tiles that need to be assigned to their grid reference

   ### Tile one large mastermap ----

   if (length(mm) == 1){  # if we have one big mastermap
      message("Tiling mastermap")

      mm <- do.call(rbind, mm)

      ## Find out which grid references polygons belong to
      index <- sf::st_intersects(sf::st_geometry(grid), sf::st_geometry(mm))  # create index
      names(index) <- grid$TILE_NAME  # add tile names
      index <- index[lengths(index) > 0]  # drop empty tiles

      # just in case there are absolutely no polygons intersecting (user loaded a tile way outside study area)
      if (length(index) == 0){
         stop("Could not find any MasterMap features in your study area.")
      }


      # Go through the spatial index and assign the tile name in a new column
      for (j in 1:length(index)){
         mm[index[[j]], "OStile"] <- names(index)[j]
      }

      # Subset the tile into a list of tiles (named by grid ref)
      mm <- split(mm, mm$OStile)

      # Remove the OStile column
      mm <- lapply(mm, function(x) dplyr::select(x, -OStile))


      rm(j)
      ## There will be duplicated polygons (along edges, that belong to two tiles) but they get deleted at a later stage.

   } else {

      ### Assign reference to existing tiles ----

      ## Index list will be as long as the number of tiles,
      ## each element being a list as long as the number of grid refs and containing indices for the tile
      index <- lapply(mm, function(x){
         sf::st_intersects(sf::st_geometry(grid), sf::st_geometry(x))  # put the test in a list
      })

      ## add names of grid ref into each list
      for (i in 1:length(index)){
         names(index[[i]]) <- grid$TILE_NAME
      }

      ## If an index element returns NO polygon at all, then this mm tile was loaded by accident and we drop it

      mm <- mm[sapply(index, function(x) sum(lengths(x))) > 0]
      index <- index[sapply(index, function(x) sum(lengths(x))) > 0] # drop from index too


      ## If the tiles are already 10 km tiles, would be nice to not reshuffle everything:
      ## Test1: Is there, for each tile, one grid ref that clearly "wins"? (more polygons than any others)
      ## Test2: Is there no winning grid ref that occurs twice? (would indicate we're dealing with smaller tiles)

      test1 <- lapply(index, function(x) max(lengths(x))/sum(lengths(x))) %>% unlist()

      test2 <- lapply(index, function(x) names(x)[lengths(x) == max(lengths(x))]) %>% unlist()

      if (all(test1 > 0.7) && all(!duplicated(test2))){

         ## Our tests are successful and we just name the mm tiles
         message("Assigning grid reference to MasterMap tiles")

         names(mm) <- test2

         rm(test1,test2)

      } else {

         ## Oh no! If the tiles don't pass our tests, they might be of different sizes and they might need serious reshuffling.
         ## This could be improved (demanding on memory) but works, so let's go an assign a grid ref to each polygon, then create the new tiles

         ## We loop through each basemap tile and produce an index of which polygon belongs to which grid ref

         for (i in 1:length(mm)){
            message("Assigning grid reference to tile ", i)
            ## For a given tile, find out which grid references polygons belong to
            index <- sf::st_intersects(sf::st_geometry(grid), sf::st_geometry(mm[[i]]))  # create index
            names(index) <- grid$TILE_NAME  # add tile names
            index <- index[lengths(index) > 0]  # drop empty tiles

            # just in case there are absolutely no polygons intersecting (user loaded a tile way outside study area)
            if (length(index) == 0){

               mm[[i]] <- NA  # "delete" content from this mm tile (but keep in position during loop)
               next           # move to next tile
            }

            # Go through the spatial index and assign the tile name in a new column

            for (j in 1:length(index)){
               mm[[i]][index[[j]], "OStile"] <- names(index)[j]
            }

            # Subset the tile into a list of tiles (named by grid ref)
            mm[[i]] <- split(mm[[i]], mm[[i]]$OStile)
         }

         ## Once this big loop is over, each mm tile should have a new column OStile with a single grid ref for each polygon. We "flatten" the mm list (remove top hierarchy) to reveal the new list structure (by grid ref) and bind those together.


         message("Recombining into 10 x 10 km grid tiles")

         mm <- do.call("c", mm)   # there is now a list element for every grid reference found in each original tile (so some grid ref appear multiple times as some polygons straddle the edges)

         ## And now we want a list where each tile has a grid ref name and contains all the polys belonging to it

         mm <- sapply(unique(names(mm)), function(x) mm[names(mm)==x] %>%
                         do.call(rbind,.), simplify=FALSE)
         # re-agregate (bind) into each unique grid reference

         mm <- mm[sapply(mm, function(x) inherits(x, "sf")) == TRUE] # remove the NAs we set earlier (create a "matrix" type object of dim 1x1)


         ### IF ABOVE DOESN'T WORK: Use the faster_intersect function to clip to study area and forget about tiles (just produce tiles if mm is one big object)


         ## Drop empty geometries after intersection (if user loaded a tile that is not within the study area)
         # Some list items can become empty after clipping and cause problems later
         mm <- mm[sapply(mm, function(x) dim(x)[1]) > 0]  # drops objects with empty geometries

         # Remove the OStile column
         mm <- lapply(mm, function(x) dplyr::select(x, -OStile))

         rm(i,j)
      }
   }

   rm(index, grid)

   ### Clip to studyAreaBuffer

   message("Clipping to study area")
   mm <- lapply(mm, function(x) faster_intersect(x, studyAreaBuffer))
## faster_intersect is 7 minutes

   ### Step 4. Deal with duplicated polygons at tiles' edges -----

   ## Where the data files have been cut by the 10 km grid, polygons that overlap the boundary are present in the two tiles. They share the common field TOID (verified with QGIS).

   ## This step should be done before converting multipart to singlepart, because TOIDs will be duplicated (rightly) when a multipart polygon is split in two

   mm <- removeDuplicPoly(mm, ID = "TOID")    # fast enough

   ### Step 5. Tidy up the geometries and calculate new fields -----

   # Doubtful that MM would have slivers, but can happen if someone has already clipped to their study area
   # Following tidying up steps from the Ecoserv Technical Report and the Word Toolkit User Guide
   message("Checking geometry validity")
   mm <- lapply(mm,
                function(x) x %>%
                   sf::st_make_valid() %>%                # repair geometry if needed
                   sf::st_cast(to = "MULTIPOLYGON") %>%   # equivalent of multi-part to single part
                   sf::st_cast(to = "POLYGON", warn = FALSE) %>% # equivalent of multi-part to single part

                   dplyr::mutate(
                      peri = as.numeric(lwgeom::st_perimeter(.)),   # calculate perimeter (same as Arc LENGTH)
                      area = as.numeric(sf::st_area(.)),
                      slvr = (pi * ((peri / (2 * pi)) ^ 2)) / area   # from technical guide
                   ) %>%
                   # eliminate slivers
                   dplyr::filter(!(area < 20 && slvr > 15 && Group != "Path")  # remove narrow things that are not paths
                   )

   )


   ### Step 6. Add a buffer for the sea (not applicable for Dane) -----

   ## Import coastline boundaries
   ## anything outside the UK boundary is labelled as sea and a buffer added...

   # Low-priority step before April


   # SAVE UPDATED MAP ----------------------------------------------------------------------------

   # Save the list of spatial objects as an RDS object (quicker to reload in R)

   saveRDS(mm, file.path(output_temp, paste0(title,"_MM_01.RDS")))

   # Update the project log with the information that map was updated

   projectLog$SAbuffer <- file.path(output_temp,
                                    paste(title, "_studyAreaBuffer_",studybuffer,"m.gpkg", sep=""))

   projectLog$last_success <- "MM_01.RDS"

   updateProjectLog(projectLog) # save revised log

   timeB <- Sys.time() # stop time

   message(paste0("MasterMap preparation finished. Process took ",
                  round(difftime(timeB, timeA, units = "mins"), digits = 1), " minutes. Ready for
                  processing."))
   return({
      ## returns the objects in the global environment
      invisible({
      mm <<- mm
      studyAreaBuffer <<- studyAreaBuffer
      })
   })

   on.exit(invisible(gc())) # garbage collection - return some memory to computer

}
